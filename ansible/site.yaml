# ansible/site.yaml
---
############################
# COMMON – ALL NODES
############################
- name: Common setup on all nodes
  hosts: all
  become: true
  vars:
    k8s_version_minor: "v1.30"
  tasks:
    - name: Disable swap (runtime)
      command: swapoff -a
      changed_when: false
      failed_when: false

    - name: Disable swap (fstab)
      replace:
        path: /etc/fstab
        regexp: '^(\s*[^#]\S+\s+\S+\s+swap\s+\S+.*)$'
        replace: '# \1'

    - name: Load kernel modules
      copy:
        dest: /etc/modules-load.d/k8s.conf
        content: |
          overlay
          br_netfilter

    - name: modprobe kernel modules
      shell: |
        modprobe overlay
        modprobe br_netfilter
      args:
        executable: /bin/bash
      changed_when: false

    - name: Sysctl for Kubernetes
      copy:
        dest: /etc/sysctl.d/k8s.conf
        content: |
          net.bridge.bridge-nf-call-iptables  = 1
          net.bridge.bridge-nf-call-ip6tables = 1
          net.ipv4.ip_forward                 = 1

    - name: Apply sysctl
      command: sysctl --system
      changed_when: false

    - name: Install base packages
      apt:
        update_cache: true
        name:
          - curl
          - ca-certificates
          - gpg
          - apt-transport-https
        state: present

    - name: Install containerd and CNI plugins
      apt:
        name:
          - containerd
          - containernetworking-plugins
        state: present

    - name: Generate default containerd config if missing
      shell: |
        set -euo pipefail
        mkdir -p /etc/containerd
        test -f /etc/containerd/config.toml || (containerd config default | tee /etc/containerd/config.toml >/dev/null)
      args:
        executable: /bin/bash

    - name: Configure containerd (SystemdCgroup)
      replace:
        path: /etc/containerd/config.toml
        regexp: 'SystemdCgroup = false'
        replace: 'SystemdCgroup = true'

    - name: Restart containerd
      systemd:
        name: containerd
        state: restarted
        enabled: true

    ############################
    # Kubernetes repo
    ############################
    - name: Ensure keyrings dir exists
      file:
        path: /etc/apt/keyrings
        state: directory
        mode: "0755"

    - name: Install Kubernetes apt keyring (simple, ipv4, timeouts)
      shell: |
        set -euo pipefail
        curl -4 -fsSL -A "Mozilla/5.0" \
          --connect-timeout 5 --max-time 20 \
          --retry 6 --retry-delay 2 --retry-connrefused \
          "https://pkgs.k8s.io/core:/stable:/{{ k8s_version_minor }}/deb/Release.key" \
          | gpg --dearmor --batch --yes -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
      args:
        executable: /bin/bash
      changed_when: true

    - name: Add Kubernetes apt repo (pkgs.k8s.io)
      copy:
        dest: /etc/apt/sources.list.d/kubernetes.list
        mode: "0644"
        content: |
          deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/{{ k8s_version_minor }}/deb/ /

    - name: Apt update (fail fast if repo blocked)
      apt:
        update_cache: true

    - name: Install kubelet, kubeadm, kubectl
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present
        update_cache: false

    ############################
    # node-ip + external cloud provider (FORÇA 10.x)
    ############################
    - name: Fail if private_ip not defined in inventory
      fail:
        msg: "private_ip não definido no inventory para {{ inventory_hostname }}"
      when: hostvars[inventory_hostname].private_ip is not defined or hostvars[inventory_hostname].private_ip | length == 0

    - name: Configure kubelet default args (node-ip private + external cloud provider)
      copy:
        dest: /etc/default/kubelet
        mode: "0644"
        content: |
          KUBELET_EXTRA_ARGS=--node-ip={{ hostvars[inventory_hostname].private_ip }} --cloud-provider=external

    - name: Reload systemd
      systemd:
        daemon_reload: true

    - name: Restart kubelet
      systemd:
        name: kubelet
        state: restarted
        enabled: true


############################
# MASTER
############################
- name: Init control-plane
  hosts: master
  become: true

  vars:
    # Versões recentes do CSI usam manifests versionados (ex: v2.11.0+).
    # Mantém fácil pinagem e upgrades controlados.
    hcloud_csi_version: "v2.11.0"
    hcloud_csi_manifest_url: "https://raw.githubusercontent.com/hetznercloud/csi-driver/{{ hcloud_csi_version }}/deploy/kubernetes/hcloud-csi.yml"

  pre_tasks:
    - name: Resolve HCLOUD_TOKEN from controller env (optional)
      set_fact:
        hcloud_token_resolved: "{{ lookup('env','HCLOUD_TOKEN') | default('') }}"
      delegate_to: localhost
      run_once: true

  tasks:
    - name: Check if cluster already initialized
      stat:
        path: /etc/kubernetes/admin.conf
      register: adminconf

    - name: Fail if private_ip not defined for master
      fail:
        msg: "private_ip não definido no inventory para {{ inventory_hostname }}"
      when:
        - not adminconf.stat.exists
        - hostvars[inventory_hostname].private_ip is not defined or hostvars[inventory_hostname].private_ip | length == 0

    - name: Get master public IPv4 (hetzner metadata)
      shell: curl -fsS http://169.254.169.254/hetzner/v1/metadata/public-ipv4
      args:
        executable: /bin/bash
      register: master_public_ip
      changed_when: false
      when: not adminconf.stat.exists

    - name: Render kubeadm config
      template:
        src: kubeadm-config.yaml.j2
        dest: /root/kubeadm-config.yaml
        mode: "0600"
      vars:
        control_plane_endpoint: "{{ hostvars[inventory_hostname].private_ip }}:6443"
        apiserver_sans:
          - "{{ hostvars[inventory_hostname].private_ip }}"
          - "{{ master_public_ip.stdout }}"
          - "127.0.0.1"
      when: not adminconf.stat.exists

    - name: kubeadm init
      command: kubeadm init --config /root/kubeadm-config.yaml
      when: not adminconf.stat.exists

    - name: Setup kubeconfig for root
      shell: |
        set -euo pipefail
        mkdir -p /root/.kube
        cp /etc/kubernetes/admin.conf /root/.kube/config
      args:
        executable: /bin/bash

    ############################
    # CNI (Flannel) + patch iface 10.x
    ############################
    - name: Install Flannel CNI
      command: kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Detect private interface name (10.x) on master
      shell: |
        set -euo pipefail
        ip -o -4 addr show scope global | awk '$4 ~ /^10\./ {print $2; exit}'
      args:
        executable: /bin/bash
      register: flannel_iface
      changed_when: false

    - name: Fail if flannel iface not found
      fail:
        msg: "Não achei interface com IP 10.x no master (necessário para Flannel)."
      when: flannel_iface.stdout | length == 0

    - name: Patch Flannel DaemonSet to use private interface (idempotent)
      shell: |
        set -euo pipefail
        if kubectl -n kube-flannel get ds kube-flannel-ds -o json | grep -q -- "--iface="; then
          exit 0
        fi
        kubectl -n kube-flannel patch ds kube-flannel-ds --type='json' -p='[
          {"op":"add","path":"/spec/template/spec/containers/0/args/-","value":"--iface={{ flannel_iface.stdout }}"}
        ]'
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      args:
        executable: /bin/bash
      changed_when: true
      failed_when: false

    - name: Restart Flannel DaemonSet
      shell: |
        set -euo pipefail
        kubectl -n kube-flannel rollout restart ds kube-flannel-ds
        kubectl -n kube-flannel rollout status ds kube-flannel-ds --timeout=240s
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      args:
        executable: /bin/bash
      changed_when: false
      failed_when: false

    ############################
    # Hetzner CCM (required for LB via Service type=LoadBalancer)
    ############################
    - name: Fail if HCLOUD_TOKEN not set on controller (required for CCM)
      fail:
        msg: "HCLOUD_TOKEN não está definido no ambiente do teu Mac (controller). Ex: export HCLOUD_TOKEN=..."
      delegate_to: localhost
      run_once: true
      when: hcloud_token_resolved | length == 0

    - name: Fail if hcloud_network_id not set (required for CCM)
      fail:
        msg: "hcloud_network_id não definido (group_vars)."
      when: hcloud_network_id is not defined or hcloud_network_id | length == 0

    - name: Ensure hcloud secret exists/updated (token + network)
      shell: |
        set -euo pipefail
        kubectl -n kube-system create secret generic hcloud \
          --from-literal=token={{ hcloud_token_resolved }} \
          --from-literal=network={{ hcloud_network_id }} \
          --dry-run=client -o yaml | kubectl apply -f -
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      args:
        executable: /bin/bash

    - name: Apply Hetzner CCM (networks)
      command: kubectl apply -f https://raw.githubusercontent.com/hetznercloud/hcloud-cloud-controller-manager/master/deploy/ccm-networks.yaml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Wait for CCM rollout
      command: kubectl -n kube-system rollout status deploy/hcloud-cloud-controller-manager --timeout=240s
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      changed_when: false

    ############################
    # Hetzner CSI (Volumes + StorageClass)
    ############################
    - name: Apply Hetzner CSI Driver
      command: kubectl apply -f {{ hcloud_csi_manifest_url }}
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Wait for CSI controller rollout (StatefulSet)
      shell: |
        set -euo pipefail
        kubectl -n kube-system rollout status statefulset/hcloud-csi-controller --timeout=240s
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      args:
        executable: /bin/bash
      changed_when: false
      failed_when: false

    - name: Wait for CSI node rollout (DaemonSet)
      shell: |
        set -euo pipefail
        kubectl -n kube-system rollout status daemonset/hcloud-csi-node --timeout=240s
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      args:
        executable: /bin/bash
      changed_when: false
      failed_when: false

    - name: Ensure Hetzner StorageClass is default (hcloud-volumes)
      shell: |
        set -euo pipefail
        # remove default from any current default storageclass (best-effort)
        for sc in $(kubectl get sc -o jsonpath='{range .items[?(@.metadata.annotations.storageclass\.kubernetes\.io/is-default-class=="true")]}{.metadata.name}{"\n"}{end}'); do
          kubectl annotate sc "$sc" storageclass.kubernetes.io/is-default-class="false" --overwrite || true
        done
        # set hetzner as default (the manifest creates hcloud-volumes)
        kubectl annotate sc hcloud-volumes storageclass.kubernetes.io/is-default-class="true" --overwrite
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      args:
        executable: /bin/bash
      changed_when: false
      failed_when: false

    ############################
    # Join command
    ############################
    - name: Get join command
      command: kubeadm token create --print-join-command
      register: join_cmd
      changed_when: false

    - name: Save join command
      set_fact:
        kubeadm_join_command: "{{ join_cmd.stdout }}"


############################
# WORKERS
############################
- name: Join workers
  hosts: workers
  become: true
  tasks:
    - name: Check if worker already joined
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubeletconf

    - name: Restart kubelet (ensure args applied)
      systemd:
        name: kubelet
        state: restarted
      when: not kubeletconf.stat.exists

    - name: Get join command from master (run once)
      command: kubeadm token create --print-join-command
      delegate_to: "{{ groups['master'][0] }}"
      run_once: true
      register: join_cmd
      changed_when: false

    - name: Join cluster
      command: "{{ join_cmd.stdout }}"
      when: not kubeletconf.stat.exists


############################
# FETCH KUBECONFIG
############################
- name: Fetch kubeconfig to local machine
  hosts: master
  become: false
  tasks:
    - name: Fetch admin.conf
      fetch:
        src: /etc/kubernetes/admin.conf
        dest: "{{ playbook_dir }}/kubeconfig/admin.conf"
        flat: true

- name: Install kubeconfig locally
  hosts: localhost
  connection: local
  gather_facts: false
  become: false
  tasks:
    - name: Ensure ~/.kube exists
      file:
        path: "{{ lookup('env','HOME') }}/.kube"
        state: directory
        mode: "0700"

    - name: Copy kubeconfig to ~/.kube
      copy:
        src: kubeconfig/admin.conf
        dest: "{{ lookup('env','HOME') }}/.kube/k8s-lab.conf"
        mode: "0600"

    - name: Export KUBECONFIG in shell profile
      lineinfile:
        path: "{{ lookup('env','HOME') }}/.zshrc"
        regexp: '^export KUBECONFIG=\$HOME/\.kube/k8s-lab\.conf$'
        line: 'export KUBECONFIG=$HOME/.kube/k8s-lab.conf'
        create: yes

    - name: Point kubeconfig to master public IP (edit file)
      replace:
        path: "{{ lookup('env','HOME') }}/.kube/k8s-lab.conf"
        regexp: 'server: https://.*:6443'
        replace: "server: https://{{ hostvars[groups['master'][0]].ansible_host }}:6443"
